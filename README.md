# PrÃ©diction de ComplÃ©tion de Cours - TensorFlow

## Description du Projet

Ce projet implÃ©mente un rÃ©seau de neurones profond (Deep Neural Network) multi-tÃ¢ches utilisant **TensorFlow/Keras** pour prÃ©dire la complÃ©tion de cours en ligne. Le modÃ¨le rÃ©sout simultanÃ©ment :

1. **RÃ©gression multi-sorties** : PrÃ©dire 4 variables continues
   - `Project_Grade` : Note du projet (0-100)
   - `Quiz_Score_Avg` : Score moyen des quiz (0-100)
   - `Progress_Percentage` : Pourcentage de progression (0-100)
   - `Satisfaction_Rating` : Note de satisfaction (1-5)

2. **Classification binaire** : PrÃ©dire si l'Ã©tudiant complÃ¨te le cours
   - `Completed` : Oui (1) / Non (0)

## Architecture du ModÃ¨le

### Architecture Multi-TÃ¢ches (Multi-Task Learning)

```
Input (n_features)
    â†“
[Shared Layers] - Tronc commun
    â”œâ”€ Dense(256) + BatchNorm + Dropout
    â”œâ”€ Dense(128) + BatchNorm + Dropout
    â”œâ”€ Dense(64) + BatchNorm + Dropout
    â””â”€ Dense(32) + BatchNorm + Dropout
    â†“
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â†“                 â†“                 â†“
[Regression Branch] [Classification Branch]
    â†“                 â†“
Dense(64,32)     Dense(64,32)
    â†“                 â†“
Output(4)        Output(1)
sigmoid          sigmoid
```

### CaractÃ©ristiques Techniques

- **Activation** : ReLU pour les couches cachÃ©es, Sigmoid pour les sorties
- **RÃ©gularisation** : 
  - Dropout (30%)
  - L2 Regularization (0.001)
  - Batch Normalization
- **Optimiseur** : Adam (learning rate = 0.001)
- **Loss Functions** :
  - RÃ©gression : Mean Squared Error (MSE)
  - Classification : Binary Cross-Entropy
- **Callbacks** :
  - Early Stopping (patience=15)
  - Reduce Learning Rate on Plateau (patience=10)
  - Model Checkpoint (sauvegarde du meilleur modÃ¨le)

## Structure du Projet

```
course-completion-prediction/
â”‚
â”œâ”€â”€ config.py                 # Configuration et constantes
â”œâ”€â”€ preprocessing.py          # PrÃ©traitement des donnÃ©es
â”œâ”€â”€ model_tensorflow.py       # ModÃ¨le TensorFlow/Keras
â”œâ”€â”€ evaluation.py             # Ã‰valuation et visualisation
â”œâ”€â”€ main.py                   # Script principal
â”œâ”€â”€ requirements.txt          # DÃ©pendances Python
â”œâ”€â”€ README.md                 # Documentation
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ Course_Completion_Prediction.csv
â”‚
â””â”€â”€ outputs/
    â”œâ”€â”€ models/               # ModÃ¨les sauvegardÃ©s
    â”œâ”€â”€ plots/                # Graphiques gÃ©nÃ©rÃ©s
    â””â”€â”€ metrics_report_*.txt  # Rapports d'Ã©valuation
```

## Installation et Utilisation

### 1. PrÃ©requis

- Python 3.8 ou supÃ©rieur
- pip

### 2. Installation des dÃ©pendances

```bash
pip install -r requirements.txt
```

### 3. ExÃ©cution du pipeline complet

```bash
python main.py
```

### Options de ligne de commande

```bash
# Avec des paramÃ¨tres personnalisÃ©s
python main.py --epochs 150 --batch-size 256 --data-path /chemin/vers/data.csv
```

## Pipeline de DonnÃ©es

### 1. PrÃ©traitement

Le module `preprocessing.py` effectue :

- **VÃ©rification de qualitÃ©** : DÃ©tection des valeurs manquantes et doublons
- **Encodage** : LabelEncoder pour les variables catÃ©gorielles
- **Normalisation** : 
  - StandardScaler pour les features
  - Min-Max normalization (0-1) pour les targets de rÃ©gression
- **Split** : 80% train / 20% test avec stratification

### 2. Features UtilisÃ©es

**Variables numÃ©riques** (19 features) :
- DÃ©mographiques : Age
- Cours : Course_Duration_Days, Instructor_Rating
- Engagement : Login_Frequency, Average_Session_Duration_Min, Video_Completion_Rate
- Interaction : Discussion_Participation, Peer_Interaction_Score
- ActivitÃ© : Time_Spent_Hours, Days_Since_Last_Login, Rewatch_Count
- Performance : Quiz_Attempts, Assignments_Missed
- Paiement : Payment_Amount, App_Usage_Percentage
- Support : Reminder_Emails_Clicked, Support_Tickets_Raised, Notifications_Checked

**Variables catÃ©gorielles** (12 features) :
- Gender, Education_Level, Employment_Status
- City, Device_Type, Internet_Connection_Quality
- Course_Name, Category, Course_Level
- Payment_Mode, Fee_Paid, Discount_Used

**Total : 31 features**

### 3. Variables Exclues

- Student_ID, Name (identifiants)
- Course_ID, Enrollment_Date (mÃ©tadonnÃ©es)
- Assignments_Submitted (risque de data leakage)
- Les 5 variables cibles

## ğŸ“ˆ Ã‰valuation

### MÃ©triques de RÃ©gression

Pour chaque target :
- **MAE** (Mean Absolute Error)
- **MSE** (Mean Squared Error)
- **RMSE** (Root Mean Squared Error)
- **RÂ²** (Coefficient de dÃ©termination)

### MÃ©triques de Classification

- **Accuracy** : Taux de prÃ©dictions correctes
- **Precision** : Proportion de vrais positifs parmi les positifs prÃ©dits
- **Recall** : Proportion de vrais positifs identifiÃ©s
- **F1-Score** : Moyenne harmonique de Precision et Recall
- **AUC-ROC** : Aire sous la courbe ROC

### Visualisations GÃ©nÃ©rÃ©es

1. **training_history.png** : Courbes d'apprentissage (loss, mÃ©triques)
2. **regression_predictions.png** : Scatter plots prÃ©dictions vs rÃ©alitÃ©
3. **regression_residuals.png** : Analyse des rÃ©sidus
4. **classification_results.png** : Matrice de confusion, courbe ROC, distribution des probabilitÃ©s

## ğŸ¯ Justifications des Choix Techniques

### Pourquoi un ModÃ¨le Multi-TÃ¢ches ?

1. **Partage de reprÃ©sentations** : Les features communes (engagement, dÃ©mographie) sont pertinentes pour les deux tÃ¢ches
2. **RÃ©gularisation implicite** : L'apprentissage simultanÃ© rÃ©duit le surapprentissage
3. **EfficacitÃ©** : Un seul modÃ¨le au lieu de 5 modÃ¨les sÃ©parÃ©s
4. **CohÃ©rence** : Les prÃ©dictions sont liÃ©es (un Ã©tudiant avec de bonnes notes a plus de chances de complÃ©ter)

### Architecture en DÃ©tail

**Tronc commun (4 couches)** :
- Extrait des features gÃ©nÃ©rales utiles aux deux tÃ¢ches
- Profondeur suffisante pour capturer des patterns complexes
- BatchNorm stabilise l'apprentissage
- Dropout Ã©vite le surapprentissage

**Branches spÃ©cialisÃ©es (2 couches chacune)** :
- Permet d'apprendre des features spÃ©cifiques Ã  chaque tÃ¢che
- Couches plus petites (64â†’32) car elles affinent les reprÃ©sentations

**Activations** :
- ReLU : Standard pour les couches cachÃ©es, Ã©vite le gradient vanishing
- Sigmoid (sorties) : AppropriÃ© pour rÃ©gression normalisÃ©e [0,1] et classification binaire

### HyperparamÃ¨tres

**Choix du Learning Rate (0.001)** :
- Valeur standard pour Adam
- Ã‰quilibre entre vitesse de convergence et stabilitÃ©
- ReduceLROnPlateau ajuste automatiquement si nÃ©cessaire

**Dropout (30%)** :
- Taux modÃ©rÃ© pour rÃ©seau profond
- PrÃ©vient le co-adaptation des neurones
- Balance rÃ©gularisation et capacitÃ© d'apprentissage

**Batch Size (128)** :
- Compromis entre :
  - StabilitÃ© du gradient (batches plus grands)
  - GÃ©nÃ©ralisation (batches plus petits)
  - Performance computationnelle

**Early Stopping (patience=15)** :
- 15 Ã©poques sans amÃ©lioration avant arrÃªt
- EmpÃªche le surapprentissage
- Ã‰conomise du temps de calcul

### Normalisation des DonnÃ©es

**StandardScaler pour les features** :
- Centre les donnÃ©es (moyenne=0, variance=1)
- Crucial pour la convergence des rÃ©seaux de neurones
- Ã‰vite que certaines features dominent

**Min-Max [0,1] pour les targets de rÃ©gression** :
- Facilite l'apprentissage avec activation sigmoid
- HomogÃ©nÃ©ise les Ã©chelles diffÃ©rentes (0-100 vs 1-5)
- AmÃ©liore la stabilitÃ© numÃ©rique

## ğŸ”§ Code de QualitÃ©

### Standards RespectÃ©s

- **PEP-8** : Formatage du code Python
- **Type hints** : Annotations de types pour clartÃ©
- **Docstrings** : Documentation complÃ¨te de chaque fonction/classe
- **ModularitÃ©** : SÃ©paration en modules logiques
- **Commentaires** : Explications des choix techniques

### Bonnes Pratiques

- Gestion des erreurs
- Logging informatif
- ReproductibilitÃ© (random_state=42)
- SÃ©paration train/validation/test
- Callbacks pour monitoring

## ğŸ“ RÃ©sultats Attendus

Le modÃ¨le gÃ©nÃ¨re automatiquement :

1. **ModÃ¨les sauvegardÃ©s** :
   - `best_model_tensorflow.h5` : Meilleur modÃ¨le pendant l'entraÃ®nement
   - `final_model_tensorflow.h5` : ModÃ¨le final

2. **Rapport de mÃ©triques** :
   - Fichier texte avec toutes les mÃ©triques
   - Configuration du modÃ¨le
   - Timestamp

3. **Visualisations** :
   - 4 graphiques PNG dÃ©taillÃ©s
   - Haute rÃ©solution (DPI=100)

## ğŸ“ Contexte AcadÃ©mique

**Projet** : ImplÃ©mentation de rÃ©seaux de neurones pour rÃ©gression + classification  
**Framework** : TensorFlow/Keras  
**Date** : DÃ©cembre 2024  
**Objectifs** :
- âœ… ImplÃ©mentation d'un MLP/DNN avec TensorFlow
- âœ… PrÃ©traitement rigoureux des donnÃ©es
- âœ… Ã‰valuation et optimisation des modÃ¨les
- âœ… Code de qualitÃ©, documentÃ© et modulaire
- âœ… Justification de toutes les dÃ©cisions techniques

## ğŸ“š RÃ©fÃ©rences

- [TensorFlow Documentation](https://www.tensorflow.org/api_docs)
- [Keras Guide](https://keras.io/guides/)
- [Multi-Task Learning](https://en.wikipedia.org/wiki/Multi-task_learning)
- [Deep Learning Book](https://www.deeplearningbook.org/)

## ğŸ‘¤ Auteur

**Keralo**  
Ã‰tudiant en Computer Science/Engineering - ESAIP  
SpÃ©cialisation : Intelligence Artificielle

---

*Ce projet dÃ©montre une comprÃ©hension approfondie des rÃ©seaux de neurones, du prÃ©traitement de donnÃ©es, et des bonnes pratiques de dÃ©veloppement en Deep Learning.*